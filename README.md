**Web Crawler**

Simple web crawler.
- Initial URL list is retrieved from txt file
- Count the number of chars in the response
- Collect and process linked pages (URLs) - don't repeat existing
- Future: build and output URLs in a graph/tree structure
- Output summary to a file. Future: use a standard log (Log4j / Logstash)
- Start with in-memory queue & tree/graph. Future: use services 

Subjects covered:
- Simple file usage
- Http calls - Using Jsoup (RegEx also an option)
- Concurrency





